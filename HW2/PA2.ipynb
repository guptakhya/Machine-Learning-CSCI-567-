{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZqS9Ll6DxQ2w"
      },
      "source": [
        "# **Linear Regression (30 points)**\n",
        "\n",
        "The data was collected and made available by “National Institute of Diabetes and Digestive and Kidney Diseases” as part of the Pima Indians Diabetes Database. Several constraints were placed on the selection of these instances from a larger database. In particular, all patients here belong to the Pima Indian heritage (subgroup of Native Americans), and are females of ages 21 and above.\n",
        "\n",
        "**There are 442 samples and 10 feature variables in this data-set.**\n",
        "\n",
        "The objective is to predict quantitative measure of disease progression one year after baseline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Z35OfoczGgl"
      },
      "source": [
        "**STEP 1: Importing all the required libraries.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bdkfY9bYzIEZ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt \n",
        "\n",
        "import pandas as pd  \n",
        "import seaborn as sns \n",
        "\n",
        "from sklearn import datasets, linear_model\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v2CKnPtqzR3B"
      },
      "source": [
        "**STEP 2: Importing Dataset**\n",
        "\n",
        "We will load the Diabetes DataSet from scikit-learn."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0SqdFHCBzQLj"
      },
      "outputs": [],
      "source": [
        "diabetes_dataset = datasets.load_diabetes()\n",
        "\n",
        "print(diabetes_dataset.data.shape)\n",
        "print(diabetes_dataset.target.shape)\n",
        "print(diabetes_dataset.DESCR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_s0_BzRn2zLE"
      },
      "source": [
        "**STEP 3: Split the Dataset (5 Points)**\n",
        "\n",
        "**TODO:** Similar to the previous assignments, split the given X and y data into X_train, X_test, y_train, y_test, using test ratio of 0.2 and random state 0."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qXM_sx9O27BD"
      },
      "outputs": [],
      "source": [
        "X = diabetes_dataset.data\n",
        "y = diabetes_dataset.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = None, None, None, None\n",
        "##### INPUT CODE HERE (~1 line of code) ######\n",
        "\n",
        "\n",
        "##############################################\n",
        "\n",
        "assert X_train.shape == (353,10)\n",
        "assert y_train.shape == (353,)\n",
        "assert X_test.shape == (89,10)\n",
        "assert y_test.shape == (89,)\n",
        "\n",
        "print(\"Size of training data= \", X_train.shape[0],\" Samples\")\n",
        "print(\"Size of testing data= \", X_test.shape[0],\" Samples\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BoBcvYjY5Ggp"
      },
      "source": [
        "**STEP 4: Fit model to training data (10 Points)**\n",
        "\n",
        "**TODO:** Create and fit the linear regressor to X_train and y_train data. You may use [this link](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) for reference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FS0mNKYo3K67"
      },
      "outputs": [],
      "source": [
        "regr = None\n",
        "\n",
        "##### INPUT CODE HERE (~2 line of code) ######\n",
        "\n",
        "\n",
        "\n",
        "##############################################\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mMz-UURw5vdw"
      },
      "source": [
        "**STEP 5: Testing the model (5 Points)**\n",
        "\n",
        "**TODO:** Find target predictions by giving **X_test** input to your model and store them in **y_pred**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L_BOtwab5p7R"
      },
      "outputs": [],
      "source": [
        "y_pred = None\n",
        "\n",
        "##### INPUT CODE HERE (~1 line of code) ######\n",
        "\n",
        "\n",
        "##############################################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_nY9--N6Gyq"
      },
      "source": [
        "**STEP 6: Model Evaluation (10 Points)**\n",
        "\n",
        "**TODO:** **Compute RMSE (Root Mean Square Error) and R2_Score for testing data**. \n",
        "\n",
        "For RMSE, you can use sklean_metrics library (make sure to have the \"squared\" attribute set to False). You can learn about RMSE from [here](https://towardsdatascience.com/what-does-rmse-really-mean-806b65f2e48e#:~:text=Root%20Mean%20Square%20Error%20(RMSE)%20is%20a%20standard%20way%20to,it%20is%20defined%20as%20follows%3A&text=This%20tells%20us%20heuristically%20that,the%20vector%20of%20observed%20values.). \n",
        "\n",
        "For R2_Score, you can use sklean_metrics library. You can learn about R2 Score from [here](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.r2_score.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_nYxBDfz6OQO"
      },
      "outputs": [],
      "source": [
        "rmse = 0\n",
        "r2 = 0\n",
        "\n",
        "##### INPUT CODE HERE (~2 line of code) ######\n",
        "\n",
        "\n",
        "##############################################\n",
        "print(\"The model performance for training set\")\n",
        "print(\"--------------------------------------\")\n",
        "print('RMSE is {}'.format(rmse))\n",
        "print('R2 score is {}'.format(r2))\n",
        "print(\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RUoSqn8I9ewY"
      },
      "source": [
        "**STEP 6: Visualise the results**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4phn_Qi982qY"
      },
      "outputs": [],
      "source": [
        "plt.scatter(y_test, y_pred)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n7X926d4AHNb"
      },
      "source": [
        "# **Support Vector Machines (55 points)**\n",
        "\n",
        "SVM is a supervised machine learning algorithm that helps in classification or regression problems. It aims to find an optimal boundary between the possible outputs.\n",
        "\n",
        "Simply put, SVM does complex data transformations depending on the selected kernel function and based on that transformations, it tries to maximize the separation boundaries between your data points depending on the labels or classes you’ve defined. The data points with the minimum distance to the hyperplane are called Support Vectors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7n2B8jMBB5Ll"
      },
      "source": [
        "## **Multiclass Classification using Support Vector Machine**\n",
        "\n",
        "In its most simple type SVM are applied on binary classification, dividing data points either in 1 or 0. For multiclass classification, the same principle is utilized after breaking down the multiclassification problem into multiple binary classification problems.\n",
        "\n",
        "The idea is to map data points to high dimensional space to gain mutual linear separation between every two classes. This is called a **One-to-One approach**, which breaks down the multiclass problem into multiple binary classification problems. A binary classifier per each pair of classes.\n",
        "\n",
        "Another approach one can use is **One-to-Rest**. In that approach, the breakdown is set to a binary classifier per each class."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cC7ZybSCctK"
      },
      "source": [
        "In **scikit-learn** one-vs-one is not default and needs to be selected explicitly (as can be seen further down in the code). One-vs-rest is set as default. It basically divides the data points in class x and rest. Consecutively a certain class is distinguished from all other classes.\n",
        "\n",
        "**Objective:**\n",
        "\n",
        "*This programming assignment will help you understand to:*\n",
        "\n",
        "\n",
        "*   Use two SVM different classifiers to show the usage of two different kernel functions; Polynomial and RBF.\n",
        "*   Tune the hyperparameters for both the classifiers. \n",
        "*   Calculate the accuracy and f1 scores to show the performance difference between the two selected kernel functions on the same data set.\n",
        "\n",
        "**In this code, we use the Iris flower data set.** That data set contains three classes of 50 instances each, where each class refers to a type of Iris plant. You can read about Iris dataset from https://archive.ics.uci.edu/ml/datasets/iris."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HXJjTdQrDGzp"
      },
      "source": [
        "**STEP 1: Importing all the required libraries.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pgc2MJTlCfc4"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn import svm, datasets\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import f1_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qD33iCfsDaNf"
      },
      "source": [
        "**STEP 2: Importing Dataset**\n",
        "\n",
        "We will load the Iris DataSet from scikit-learn. For better understanding and visualisation in a 2D plane, we'll use the first two features as X (sepal length and sepal width of the iris flowers)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lvvf4Rd_DXwv"
      },
      "outputs": [],
      "source": [
        "iris = datasets.load_iris()\n",
        "X = iris.data[:, :2]\n",
        "y = iris.target"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GuNqeQzzBVOe"
      },
      "source": [
        "now let's divide the dataset into training and test data. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ka3xnc3pBd2V"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FbdSkwZDED3F"
      },
      "source": [
        "\n",
        "Learning the parameters of a prediction function and testing it on the same data is a methodological mistake: a model that would just repeat the labels of the samples that it has just seen would have a perfect score but would fail to predict anything useful on yet-unseen data. **This situation is called overfitting.** To avoid it, it is common practice when performing a (supervised) machine learning experiment to hold out part of the available data as a test set X_test, y_test.\n",
        "\n",
        "Similarly, when evaluating different settings (“hyperparameters”) for estimators, such as the C setting that must be manually set for an SVM, there is still a risk of overfitting on the test set because the parameters can be tweaked until the estimator performs optimally. This way, knowledge about the test set can “leak” into the model and evaluation metrics no longer report on generalization performance. To solve this problem, yet another part of the dataset can be held out as a so-called “**validation set**”: training proceeds on the training set, after which evaluation is done on the validation set, and when the experiment seems to be successful, final evaluation can be done on the test set.\n",
        "\n",
        "You can read more about it from [here](https://scikit-learn.org/stable/modules/cross_validation.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gR46FlKBEaaZ"
      },
      "source": [
        "**STEP 3: Hyperparameter Tuning (20 Points)**\n",
        "\n",
        "**TODO:** Create two objects from SVM, to create two different classifiers; one with **Polynomial kernel**, and another one with **RBF kernel**. You can read about Polynomial and RBF Kernel from [here](https://scikit-learn.org/stable/modules/svm.html).\n",
        "\n",
        "\n",
        "\n",
        "**TODO:** For both RBF and Polynomial Kernel, you are required to tune the hyperparameters. \n",
        "\n",
        "The [hyperparameters](https://towardsdatascience.com/svm-hyperparameters-explained-with-visualizations-143e48cb701b) you'll be tuning are as follows:\n",
        "\n",
        "*   For **RBF kernel**, you'll be tuning 'gamma' and 'penalty term (C)'. \n",
        "*   For **Polynomial kernel**, you'll be tuning 'degree' and 'penalty term (C)'. \n",
        "\n",
        "For the multiclass classification, the type one-versus-one will be specified, i.e. use decision_function_shape=’ovo’.\n",
        "\n",
        "All other parameters are set to default. \n",
        "\n",
        "*NOTE: We select the hyperparameters that give us highest accuracy.*\n",
        "\n",
        "**RBF kernel**\n",
        "\n",
        "1. Penalty term, C,: Try values in the range of 0.2 to 1.4 (inclusive) with an increment of 0.2.\n",
        "2. Penalty term, gamma,: Try values in the range of 0.4 to 1.6 (inclusive) with an increment of 0.2.\n",
        "\n",
        "**Polynomial kernel**\n",
        "\n",
        "1. Penalty term, C,: Try values in the range of 0.2 to 1.4 (inclusive) with an increment of 0.2.\n",
        "2. Penalty term, degree,: Try values 1 to 5 (inclusive).\n",
        "\n",
        "\n",
        "STEPS:\n",
        "\n",
        "1. For each possible combination of C and Gamma (rbf kernel) or C and Degree (poly kernel) as given above, compute the accuracy of your model using\n",
        "\n",
        "   ` cross_val_score(classifier_instance, X_train, y_train, cv=5).mean()`\n",
        "\n",
        "2. Find the hyperparameters, which give you the best accuracy. \n",
        "\n",
        "3. Use that hyperparameter for fitting the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DRYRnnXeEPvA"
      },
      "outputs": [],
      "source": [
        "##### INPUT CODE HERE (~20 line of code) ######\n",
        "\n",
        "c_rbf, gamma_rbf = 0, 0\n",
        "### RBF Kernel    ~ input code here    ~10 lines of code\n",
        "\n",
        "\n",
        "##############################################\n",
        "print(\"Best pair of hyperparameters (C, Gamma) for RBF Kernel is (\" + str(c_rbf) + \", \" + str(gamma_rbf) + \")\")\n",
        "\n",
        "d_poly, c_poly = 0, 0\n",
        "### Polynomial Kernel   ~ input code here   ~ 10 lines of code\n",
        "\n",
        "\n",
        "\n",
        "##############################################\n",
        "print(\"Best pair of hyperparameters (Degree, Gamma) for Poly Kernel is (\" + str(d_poly) + \", \" + str(c_poly) + \")\")\n",
        "##############################################\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zpxNkwhpBqXG"
      },
      "source": [
        "Now, let us create two objects from SVM, to create two different classifiers; one with Polynomial kernel, and another one with RBF kernel. *Use the best pair of hyperparameters as found above.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GX9nRyXRB6lG"
      },
      "outputs": [],
      "source": [
        "rbf = svm.SVC(kernel='rbf', gamma=gamma_rbf, C=c_rbf, decision_function_shape='ovo')\n",
        "poly = svm.SVC(kernel='poly', degree=d_poly, C=c_poly, decision_function_shape='ovo')\n",
        "\n",
        "rbf.fit(X_train, y_train)\n",
        "poly.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "noySXLUGFhUT"
      },
      "source": [
        "**STEP 5: Testing the model (10 Points)**\n",
        "\n",
        "**TODO:** Find target predictions by giving X_test input to your two models and store them in y_pred."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ll1f-gpwFNhx"
      },
      "outputs": [],
      "source": [
        "poly_pred, rbf_pred = None, None\n",
        "##### INPUT CODE HERE (~2 line of code) ######\n",
        "\n",
        "\n",
        "##############################################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YAVVUvM8F79Q"
      },
      "source": [
        "**STEP 6: Models Evaluation (20 Points)**\n",
        "\n",
        "**TODO:**\n",
        "\n",
        "*   Calculate the accuracy and f1 scores for SVM with RBF kernel\n",
        "*   Calculate the accuracy and f1 scores for SVM with Polynomial kernel\n",
        "\n",
        "Out of the known metrics for validating machine learning models, we choose Accuracy and F1 as they are the most used in supervised machine learning.\n",
        "\n",
        "For the **accuracy score**, it shows the percentage of the true positive and true negative to all data points. So, it’s useful when the data set is balanced.\n",
        "[Accuracy Score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html)\n",
        "\n",
        "\n",
        "For the **f1 score**, it calculates the harmonic mean between precision and recall, and both depend on the false positive and false negative. So, it’s useful to calculate the f1 score when the data set isn’t balanced. **While calculating the F1 score for rbf and polynomial kernels make sure to use weighted averages.**\n",
        "[F1 Score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ZAnpEAOGM9w"
      },
      "outputs": [],
      "source": [
        "rbf_accuracy, rbf_f1 = None, None\n",
        "##### INPUT CODE HERE (~2 line of code) ######\n",
        "\n",
        "\n",
        "\n",
        "##############################################\n",
        "# the output for accuracy and f1 will be in percentage\n",
        "\n",
        "print('Accuracy (RBF Kernel): ', \"%.2f\" % (rbf_accuracy*100))\n",
        "print('F1 (RBF Kernel): ', \"%.2f\" % (rbf_f1*100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XwjDeDMwGjVU"
      },
      "outputs": [],
      "source": [
        "poly_accuracy = None\n",
        "poly_f1 = None\n",
        "\n",
        "##### INPUT CODE HERE (~2 line of code) ######\n",
        "\n",
        "\n",
        "##############################################\n",
        "# the output for accuracy and f1 will be in percentage\n",
        "\n",
        "print('Accuracy (Polynomial Kernel): ', \"%.2f\" % (poly_accuracy*100))\n",
        "print('F1 (Polynomial Kernel): ', \"%.2f\" % (poly_f1*100))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Kck-9KuHHjq"
      },
      "source": [
        "**STEP 7: Visualising Results (5 points)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xLIRk_evGuEb"
      },
      "outputs": [],
      "source": [
        "#stepsize in the mesh, it alters the accuracy of the plotprint\n",
        "#to better understand it, just play with the value, change it and print it\n",
        "h = .01\n",
        "#create the mesh\n",
        "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),np.arange(y_min, y_max, h))\n",
        "# create the title that will be shown on the plot\n",
        "titles = ['RBF kernel','Polynomial kernel']\n",
        "\n",
        "for i, clf in enumerate((rbf, poly)):\n",
        "    #defines how many plots: 2 rows, 2columns=> leading to 4 plots\n",
        "    plt.subplot(2, 2, i + 1) #i+1 is the index\n",
        "    #space between plots\n",
        "    plt.subplots_adjust(wspace=0.4, hspace=0.4) \n",
        "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "    # Put the result into a color plot\n",
        "    Z = Z.reshape(xx.shape)\n",
        "    plt.contourf(xx, yy, Z, cmap=plt.cm.PuBuGn, alpha=0.7)\n",
        "    # Plot also the training points\n",
        "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.PuBuGn,     edgecolors='grey')\n",
        "    plt.xlabel('Sepal length')\n",
        "    plt.ylabel('Sepal width')\n",
        "    plt.xlim(xx.min(), xx.max())\n",
        "    plt.ylim(yy.min(), yy.max())\n",
        "    plt.xticks(())\n",
        "    plt.yticks(())\n",
        "    plt.title(titles[i])\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **[3 points]** \n",
        "\n",
        "**TO DO:** Plot confusion matrix of model predictions for the **polynomial kernel**. \n",
        "\n",
        "\n",
        "**Make sure to have the correct attribute values for the \"confusion_matrix\" and \"ConfusionMatrixDisplay\" functions.**\n",
        "\n",
        "\n",
        "\n",
        "You may need the following references:\n",
        "\n",
        "[Confusion Matrix](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html)\n",
        "\n",
        "[Confusion Matrix Display](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.ConfusionMatrixDisplay.html)\n"
      ],
      "metadata": {
        "id": "X_Dhepi4CnQn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6lR0qKNAH8v6"
      },
      "outputs": [],
      "source": [
        "# creating a confusion matrix: polynomial kernel\n",
        "\n",
        "##### INPUT CODE HERE (~3 line of code) ######\n",
        "\n",
        "\n",
        "# make sure to run this cell and show the output when submitting "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **[2 Points]**\n",
        "\n",
        "**TO DO:** Plot confusion matrix of model predictions for the **rbf kernel**. \n",
        "\n",
        "**Make sure to have the correct attribute values for the \"confusion_matrix\" and \"ConfusionMatrixDisplay\" functions**. \n",
        "\n",
        "\n",
        "You may need the following references:\n",
        "\n",
        "[Confusion Matrix](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html)\n",
        "\n",
        "[Confusion Matrix Display](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.ConfusionMatrixDisplay.html)\n"
      ],
      "metadata": {
        "id": "Q-vDWRnlDT-l"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gez_1vpDILLO"
      },
      "outputs": [],
      "source": [
        "# creating a confusion matrix: rbf kernel\n",
        "\n",
        "##### INPUT CODE HERE (~3 line of code) ######\n",
        "\n",
        "\n",
        "# make sure to run this cell and show the output when submitting "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Logistic Regression (15 points)**\n",
        "\n"
      ],
      "metadata": {
        "id": "5WwQ4qczZF3O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression"
      ],
      "metadata": {
        "id": "_Nkx73GCZFgk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the same dataset distribution done for SVM, perform the following:\n",
        "\n",
        "1.   Create a Logistic Regression Classifier with a L2 penalty, random state set to 0, and multi_class set to auto.\n",
        "2.   Fit the Classifier on X_train, y_train present from the SVM dataset. \n",
        "3.   Calculate and print the classifier's score.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "NOCU3rdYZ2EV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classifier = None\n",
        "classifier_score = 0\n",
        "##### INPUT CODE HERE (~3 line of code) ######\n",
        "\n",
        "\n",
        "\n",
        "##############################################\n",
        "# the output score will be in form of percent\n",
        "\n",
        "print('Logistic Regression Classifier Score : ', \"%.2f\" % (classifier_score*100))\n"
      ],
      "metadata": {
        "id": "DSHasUyZZnOH"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "PA2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}