{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN Assignment.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "Fm8_bkGFIVsw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**3 CONV LAYERS, 2 DROPOUT LAYERS, 1 MAXPOOL**"
      ]
    },
    {
      "metadata": {
        "id": "P1VkonFoL-3G",
        "colab_type": "code",
        "outputId": "36404f5b-aba0-4739-837f-2dcd83fccd7f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1020
        }
      },
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "import keras\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras import backend as K\n",
        "\n",
        "batch_size = 128\n",
        "num_classes = 10\n",
        "epochs = 12\n",
        "\n",
        "# input image dimensions\n",
        "img_rows, img_cols = 28, 28\n",
        "\n",
        "# the data, split between train and test sets\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "if K.image_data_format() == 'channels_first':\n",
        "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
        "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
        "    input_shape = (1, img_rows, img_cols)\n",
        "else:\n",
        "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
        "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
        "    input_shape = (img_rows, img_cols, 1)\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "print('x_train shape:', x_train.shape)\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')\n",
        "\n",
        "# convert class vectors to binary class matrices\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, kernel_size=(3, 3),\n",
        "                 activation='relu',\n",
        "                 input_shape=input_shape))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "model.compile(loss=keras.losses.categorical_crossentropy,\n",
        "              optimizer=keras.optimizers.Adadelta(),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(x_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          verbose=1,\n",
        "          validation_data=(x_test, y_test))\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "x_train shape: (60000, 28, 28, 1)\n",
            "60000 train samples\n",
            "10000 test samples\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/12\n",
            "60000/60000 [==============================] - 523s 9ms/step - loss: 0.2131 - acc: 0.9336 - val_loss: 0.0404 - val_acc: 0.9875\n",
            "Epoch 2/12\n",
            "60000/60000 [==============================] - 525s 9ms/step - loss: 0.0613 - acc: 0.9822 - val_loss: 0.0354 - val_acc: 0.9882\n",
            "Epoch 3/12\n",
            "60000/60000 [==============================] - 522s 9ms/step - loss: 0.0467 - acc: 0.9860 - val_loss: 0.0337 - val_acc: 0.9884\n",
            "Epoch 4/12\n",
            "60000/60000 [==============================] - 524s 9ms/step - loss: 0.0356 - acc: 0.9893 - val_loss: 0.0269 - val_acc: 0.9914\n",
            "Epoch 5/12\n",
            "60000/60000 [==============================] - 518s 9ms/step - loss: 0.0293 - acc: 0.9909 - val_loss: 0.0227 - val_acc: 0.9928\n",
            "Epoch 6/12\n",
            "60000/60000 [==============================] - 517s 9ms/step - loss: 0.0244 - acc: 0.9929 - val_loss: 0.0274 - val_acc: 0.9909\n",
            "Epoch 7/12\n",
            "60000/60000 [==============================] - 539s 9ms/step - loss: 0.0200 - acc: 0.9937 - val_loss: 0.0212 - val_acc: 0.9935\n",
            "Epoch 8/12\n",
            "60000/60000 [==============================] - 562s 9ms/step - loss: 0.0188 - acc: 0.9940 - val_loss: 0.0240 - val_acc: 0.9929\n",
            "Epoch 9/12\n",
            "60000/60000 [==============================] - 543s 9ms/step - loss: 0.0159 - acc: 0.9952 - val_loss: 0.0228 - val_acc: 0.9931\n",
            "Epoch 10/12\n",
            "60000/60000 [==============================] - 514s 9ms/step - loss: 0.0147 - acc: 0.9955 - val_loss: 0.0245 - val_acc: 0.9931\n",
            "Epoch 11/12\n",
            "60000/60000 [==============================] - 513s 9ms/step - loss: 0.0117 - acc: 0.9964 - val_loss: 0.0284 - val_acc: 0.9933\n",
            "Epoch 12/12\n",
            "60000/60000 [==============================] - 521s 9ms/step - loss: 0.0116 - acc: 0.9965 - val_loss: 0.0259 - val_acc: 0.9923\n",
            "Test loss: 0.025863875662055353\n",
            "Test accuracy: 0.9923\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "1_x3tyigd9ca",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "final : 99.23\n",
        "highest: 99.33\n"
      ]
    },
    {
      "metadata": {
        "id": "YagtMFoye2Wy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**5 CONV LAYERS, 3 DROPOUT LAYERS, 1 MAXPOOL**"
      ]
    },
    {
      "metadata": {
        "id": "w7XoORRAer6J",
        "colab_type": "code",
        "outputId": "ea76a27f-d2ca-40b2-9bcb-38663db3c2a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 943
        }
      },
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "import keras\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras import backend as K\n",
        "\n",
        "batch_size = 128\n",
        "num_classes = 10\n",
        "epochs = 12\n",
        "\n",
        "# input image dimensions\n",
        "img_rows, img_cols = 28, 28\n",
        "\n",
        "# the data, split between train and test sets\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "if K.image_data_format() == 'channels_first':\n",
        "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
        "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
        "    input_shape = (1, img_rows, img_cols)\n",
        "else:\n",
        "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
        "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
        "    input_shape = (img_rows, img_cols, 1)\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "print('x_train shape:', x_train.shape)\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')\n",
        "\n",
        "# convert class vectors to binary class matrices\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, kernel_size=(5, 5),\n",
        "                 activation='relu',\n",
        "                 input_shape=input_shape))\n",
        "model.add(Conv2D(64, (5, 5), activation='relu'))\n",
        "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
        "model.add(Conv2D(188, (3, 3), activation='relu'))\n",
        "#model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Conv2D(240, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(356, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "model.compile(loss=keras.losses.categorical_crossentropy,\n",
        "              optimizer=keras.optimizers.Adadelta(),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(x_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          verbose=1,\n",
        "          validation_data=(x_test, y_test))\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "x_train shape: (60000, 28, 28, 1)\n",
            "60000 train samples\n",
            "10000 test samples\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/12\n",
            "60000/60000 [==============================] - 1452s 24ms/step - loss: 0.3635 - acc: 0.8861 - val_loss: 0.0644 - val_acc: 0.9807\n",
            "Epoch 2/12\n",
            "60000/60000 [==============================] - 1452s 24ms/step - loss: 0.0829 - acc: 0.9766 - val_loss: 0.0400 - val_acc: 0.9890\n",
            "Epoch 3/12\n",
            "60000/60000 [==============================] - 1460s 24ms/step - loss: 0.0595 - acc: 0.9838 - val_loss: 0.0277 - val_acc: 0.9916\n",
            "Epoch 4/12\n",
            "60000/60000 [==============================] - 1458s 24ms/step - loss: 0.0453 - acc: 0.9872 - val_loss: 0.0317 - val_acc: 0.9903\n",
            "Epoch 5/12\n",
            "60000/60000 [==============================] - 1468s 24ms/step - loss: 0.0364 - acc: 0.9900 - val_loss: 0.0315 - val_acc: 0.9911\n",
            "Epoch 6/12\n",
            "60000/60000 [==============================] - 1493s 25ms/step - loss: 0.0337 - acc: 0.9901 - val_loss: 0.0317 - val_acc: 0.9914\n",
            "Epoch 7/12\n",
            "60000/60000 [==============================] - 1501s 25ms/step - loss: 0.0275 - acc: 0.9926 - val_loss: 0.0241 - val_acc: 0.9927\n",
            "Epoch 8/12\n",
            "60000/60000 [==============================] - 1497s 25ms/step - loss: 0.0244 - acc: 0.9928 - val_loss: 0.0366 - val_acc: 0.9906\n",
            "Epoch 9/12\n",
            "60000/60000 [==============================] - 1492s 25ms/step - loss: 0.0217 - acc: 0.9938 - val_loss: 0.0257 - val_acc: 0.9933\n",
            "Epoch 10/12\n",
            "60000/60000 [==============================] - 1496s 25ms/step - loss: 0.0190 - acc: 0.9944 - val_loss: 0.0263 - val_acc: 0.9939\n",
            "Epoch 11/12\n",
            "60000/60000 [==============================] - 1489s 25ms/step - loss: 0.0187 - acc: 0.9947 - val_loss: 0.0285 - val_acc: 0.9936\n",
            "Epoch 12/12\n",
            "60000/60000 [==============================] - 1507s 25ms/step - loss: 0.0166 - acc: 0.9951 - val_loss: 0.0225 - val_acc: 0.9939\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Ko5D0gOSTOQN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "final: 99.39\n",
        "highest: 99.39"
      ]
    },
    {
      "metadata": {
        "id": "xGY-O9CJVke2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**7 CONV LAYERS, 3 DROPOUT, 2 BN, 2 MAXPOOL**"
      ]
    },
    {
      "metadata": {
        "id": "VYnPS8rVVpO4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1020
        },
        "outputId": "a5826cf1-ded9-44bd-8916-49d5dcad642f"
      },
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "import keras\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras import backend as K\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "\n",
        "batch_size = 128\n",
        "num_classes = 10\n",
        "epochs = 12\n",
        "\n",
        "# input image dimensions\n",
        "img_rows, img_cols = 28, 28\n",
        "\n",
        "# the data, split between train and test sets\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "if K.image_data_format() == 'channels_first':\n",
        "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
        "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
        "    input_shape = (1, img_rows, img_cols)\n",
        "else:\n",
        "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
        "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
        "    input_shape = (img_rows, img_cols, 1)\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "print('x_train shape:', x_train.shape)\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')\n",
        "\n",
        "# convert class vectors to binary class matrices\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, kernel_size=(5, 5),\n",
        "                 activation='relu',\n",
        "                 input_shape=input_shape))\n",
        "model.add(Conv2D(64, (5, 5), activation='relu'))\n",
        "model.add(Conv2D(128, (5, 5), activation='relu'))\n",
        "model.add(Conv2D(168, (3, 3), activation='relu'))\n",
        "#model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(190, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Conv2D(220, (3, 3), activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(256, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(356, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(236, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "model.compile(loss=keras.losses.categorical_crossentropy,\n",
        "              optimizer=keras.optimizers.Adadelta(),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(x_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          verbose=1,\n",
        "          validation_data=(x_test, y_test))\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "x_train shape: (60000, 28, 28, 1)\n",
            "60000 train samples\n",
            "10000 test samples\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/12\n",
            "60000/60000 [==============================] - 1564s 26ms/step - loss: 0.4199 - acc: 0.8819 - val_loss: 1.2687 - val_acc: 0.8333\n",
            "Epoch 2/12\n",
            "60000/60000 [==============================] - 1579s 26ms/step - loss: 0.1967 - acc: 0.9605 - val_loss: 0.3084 - val_acc: 0.9288\n",
            "Epoch 3/12\n",
            "60000/60000 [==============================] - 1573s 26ms/step - loss: 0.1538 - acc: 0.9707 - val_loss: 0.0723 - val_acc: 0.9819\n",
            "Epoch 4/12\n",
            "60000/60000 [==============================] - 1573s 26ms/step - loss: 0.1286 - acc: 0.9750 - val_loss: 0.0777 - val_acc: 0.9790\n",
            "Epoch 5/12\n",
            "60000/60000 [==============================] - 1574s 26ms/step - loss: 0.1289 - acc: 0.9766 - val_loss: 0.0865 - val_acc: 0.9825\n",
            "Epoch 6/12\n",
            "60000/60000 [==============================] - 1579s 26ms/step - loss: 0.1225 - acc: 0.9786 - val_loss: 0.0852 - val_acc: 0.9790\n",
            "Epoch 7/12\n",
            "60000/60000 [==============================] - 1587s 26ms/step - loss: 0.1142 - acc: 0.9811 - val_loss: 0.0906 - val_acc: 0.9808\n",
            "Epoch 8/12\n",
            "60000/60000 [==============================] - 1588s 26ms/step - loss: 0.0917 - acc: 0.9832 - val_loss: 0.0654 - val_acc: 0.9853\n",
            "Epoch 9/12\n",
            "60000/60000 [==============================] - 1594s 27ms/step - loss: 0.1023 - acc: 0.9821 - val_loss: 0.0789 - val_acc: 0.9829\n",
            "Epoch 10/12\n",
            "60000/60000 [==============================] - 1605s 27ms/step - loss: 0.0950 - acc: 0.9839 - val_loss: 0.0610 - val_acc: 0.9840\n",
            "Epoch 11/12\n",
            "60000/60000 [==============================] - 1602s 27ms/step - loss: 0.0935 - acc: 0.9854 - val_loss: 0.0668 - val_acc: 0.9879\n",
            "Epoch 12/12\n",
            "60000/60000 [==============================] - 1602s 27ms/step - loss: 0.0839 - acc: 0.9861 - val_loss: 0.0554 - val_acc: 0.9877\n",
            "Test loss: 0.05537127718601228\n",
            "Test accuracy: 0.9877\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Unn-8e5SeKKD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "FINAL TEST ACCURACY: 98.77\n",
        "HIGHEST TEST ACCURACY: 98.79"
      ]
    },
    {
      "metadata": {
        "id": "Jf5eVblKcdup",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**7 CONV LAYERS, 3 DROPOUT, 1 BN, 2 MAXPOOL**"
      ]
    },
    {
      "metadata": {
        "id": "wWUI04e4chYa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 633
        },
        "outputId": "9fe865da-4604-40b5-b439-033862659f8b"
      },
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "import keras\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras import backend as K\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "\n",
        "batch_size = 128\n",
        "num_classes = 10\n",
        "epochs = 12\n",
        "\n",
        "# input image dimensions\n",
        "img_rows, img_cols = 28, 28\n",
        "\n",
        "# the data, split between train and test sets\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "if K.image_data_format() == 'channels_first':\n",
        "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
        "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
        "    input_shape = (1, img_rows, img_cols)\n",
        "else:\n",
        "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
        "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
        "    input_shape = (img_rows, img_cols, 1)\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "print('x_train shape:', x_train.shape)\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')\n",
        "\n",
        "# convert class vectors to binary class matrices\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, kernel_size=(5, 5),\n",
        "                 activation='relu',\n",
        "                 input_shape=input_shape))\n",
        "model.add(Conv2D(64, (5, 5), activation='relu'))\n",
        "model.add(Conv2D(128, (5, 5), activation='relu'))\n",
        "model.add(Conv2D(168, (3, 3), activation='relu'))\n",
        "#model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "#model.add(BatchNormalization())\n",
        "model.add(Conv2D(190, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Conv2D(220, (3, 3), activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(256, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(356, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(216, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "model.compile(loss=keras.losses.categorical_crossentropy,\n",
        "              optimizer=keras.optimizers.Adadelta(),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(x_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          verbose=1,\n",
        "          validation_data=(x_test, y_test))\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train shape: (60000, 28, 28, 1)\n",
            "60000 train samples\n",
            "10000 test samples\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/12\n",
            "60000/60000 [==============================] - 1435s 24ms/step - loss: 0.8435 - acc: 0.7328 - val_loss: 0.5227 - val_acc: 0.8355\n",
            "Epoch 2/12\n",
            "60000/60000 [==============================] - 1436s 24ms/step - loss: 0.4211 - acc: 0.8753 - val_loss: 0.3865 - val_acc: 0.8759\n",
            "Epoch 3/12\n",
            "60000/60000 [==============================] - 1427s 24ms/step - loss: 0.3599 - acc: 0.8942 - val_loss: 0.2229 - val_acc: 0.9320\n",
            "Epoch 4/12\n",
            "60000/60000 [==============================] - 1427s 24ms/step - loss: 0.3306 - acc: 0.9040 - val_loss: 0.2225 - val_acc: 0.9294\n",
            "Epoch 5/12\n",
            "22144/60000 [==========>...................] - ETA: 14:19 - loss: 0.3172 - acc: 0.9093"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-a81d8e4e600b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     70\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m           \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m           validation_data=(x_test, y_test))\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Test loss:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "urxwj4HJeDIq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "FINAL:\n",
        "HIGHEST:"
      ]
    },
    {
      "metadata": {
        "id": "ME6gL2yUJU8q",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**7 CONV LAYERS, 2 DROPOUT, 1 BN, 1 MAXPOOL**"
      ]
    },
    {
      "metadata": {
        "id": "YF1ZzJ52_hGs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1020
        },
        "outputId": "2cb60517-003e-4321-e6af-6498735e4983"
      },
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "import keras\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras import backend as K\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "\n",
        "batch_size = 128\n",
        "num_classes = 10\n",
        "epochs = 12\n",
        "\n",
        "# input image dimensions\n",
        "img_rows, img_cols = 28, 28\n",
        "\n",
        "# the data, split between train and test sets\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "if K.image_data_format() == 'channels_first':\n",
        "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
        "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
        "    input_shape = (1, img_rows, img_cols)\n",
        "else:\n",
        "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
        "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
        "    input_shape = (img_rows, img_cols, 1)\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "print('x_train shape:', x_train.shape)\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')\n",
        "\n",
        "# convert class vectors to binary class matrices\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, kernel_size=(5, 5),\n",
        "                 activation='relu',\n",
        "                 input_shape=input_shape))\n",
        "model.add(Conv2D(64, (5, 5), activation='relu'))\n",
        "model.add(Conv2D(128, (5, 5), activation='relu'))\n",
        "model.add(Conv2D(168, (3, 3), activation='relu'))\n",
        "#model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "#model.add(BatchNormalization())\n",
        "model.add(Conv2D(120, (3, 3), activation='relu'))\n",
        "#model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Conv2D(240, (3, 3), activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(286, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "#model.add(Dropout(0.25))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(386, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(246, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "model.compile(loss=keras.losses.categorical_crossentropy,\n",
        "              optimizer=keras.optimizers.Adadelta(),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(x_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          verbose=1,\n",
        "          validation_data=(x_test, y_test))\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 1s 0us/step\n",
            "x_train shape: (60000, 28, 28, 1)\n",
            "60000 train samples\n",
            "10000 test samples\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/12\n",
            "60000/60000 [==============================] - 1756s 29ms/step - loss: 0.7360 - acc: 0.7838 - val_loss: 0.2368 - val_acc: 0.9349\n",
            "Epoch 2/12\n",
            "60000/60000 [==============================] - 1753s 29ms/step - loss: 0.2295 - acc: 0.9398 - val_loss: 0.2254 - val_acc: 0.9393\n",
            "Epoch 3/12\n",
            "60000/60000 [==============================] - 1754s 29ms/step - loss: 0.1514 - acc: 0.9613 - val_loss: 0.1594 - val_acc: 0.9554\n",
            "Epoch 4/12\n",
            "60000/60000 [==============================] - 1756s 29ms/step - loss: 0.1221 - acc: 0.9684 - val_loss: 0.0787 - val_acc: 0.9767\n",
            "Epoch 5/12\n",
            "60000/60000 [==============================] - 1753s 29ms/step - loss: 0.1033 - acc: 0.9730 - val_loss: 0.0760 - val_acc: 0.9808\n",
            "Epoch 6/12\n",
            "60000/60000 [==============================] - 1752s 29ms/step - loss: 0.1009 - acc: 0.9749 - val_loss: 0.0855 - val_acc: 0.9782\n",
            "Epoch 7/12\n",
            "60000/60000 [==============================] - 1756s 29ms/step - loss: 0.0880 - acc: 0.9780 - val_loss: 0.0938 - val_acc: 0.9753\n",
            "Epoch 8/12\n",
            "60000/60000 [==============================] - 1759s 29ms/step - loss: 0.0813 - acc: 0.9803 - val_loss: 0.0635 - val_acc: 0.9836\n",
            "Epoch 9/12\n",
            "60000/60000 [==============================] - 1755s 29ms/step - loss: 0.0776 - acc: 0.9811 - val_loss: 0.0868 - val_acc: 0.9790\n",
            "Epoch 10/12\n",
            "60000/60000 [==============================] - 1791s 30ms/step - loss: 0.0723 - acc: 0.9818 - val_loss: 0.0590 - val_acc: 0.9844\n",
            "Epoch 11/12\n",
            "60000/60000 [==============================] - 1790s 30ms/step - loss: 0.0732 - acc: 0.9830 - val_loss: 0.0781 - val_acc: 0.9816\n",
            "Epoch 12/12\n",
            "60000/60000 [==============================] - 1771s 30ms/step - loss: 0.0679 - acc: 0.9837 - val_loss: 0.0623 - val_acc: 0.9851\n",
            "Test loss: 0.06231217963733657\n",
            "Test accuracy: 0.9851\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "RF3Z1V2-hYs4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Final test accuracy: 98.51\n",
        "Highest test accuracy: 98.51\n"
      ]
    },
    {
      "metadata": {
        "id": "GW55ZgbIhkcE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**CONCLUSION**\n",
        "The final accuracy for 3 conv layered model is 99.23. That for 5 conv layered model was 99.39, an increase of 0.16%. In 7 conv layered model, the best outcome was achieved with a model containing 7 CONV LAYERS, 3 DROPOUT, 2 BN, 2 MAXPOOL layers, with a final test accuracy of 98.77. With general expectations, the accuracy should have increased in the 7 conv layers model. However, as we studied, increasing the number of layers can sometimes cause loss of information and is mended by using residual networks."
      ]
    }
  ]
}
